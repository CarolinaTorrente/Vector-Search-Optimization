{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClusterClassifier:\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_parquet(\"df_final.parquet\")\n",
    "\n",
    "    def classify_embedding(self, embedding: list[float]) -> int:\n",
    "        # columns: [[\"cluster_label\", \"etiqueta\", \"centroid\"]]\n",
    "        distances = cdist([embedding], self.df.centroid.tolist())\n",
    "        return self.df.iloc[np.argmin(distances)][\"cluster_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../df_final.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distances = cdist([df.centroid.iloc[0]], df.centroid.tolist())\n",
    "distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[np.argmin(distances)][\"cluster_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el DataFrame\n",
    "df_final = pd.read_parquet(\"../df_final.parquet\")\n",
    "df_final[\"etiqueta\"] = \"minuscula\"\n",
    "\n",
    "# Comprobar la capitalización\n",
    "print(\"Primeras 5 etiquetas originales:\")\n",
    "print(df_final[\"etiqueta\"].head())\n",
    "\n",
    "print(\"\\nPrimeras 5 etiquetas capitalizadas:\")\n",
    "print(df_final[\"etiqueta\"].str.capitalize().head())\n",
    "\n",
    "# Verificar si todas las etiquetas están capitalizadas\n",
    "todas_capitalizadas = df_final[\"etiqueta\"].str[0].str.isupper().all()\n",
    "print(f\"\\n¿Todas las etiquetas están capitalizadas? {todas_capitalizadas}\")\n",
    "\n",
    "# Mostrar etiquetas no capitalizadas (si las hay)\n",
    "if not todas_capitalizadas:\n",
    "    print(\"\\nEtiquetas no capitalizadas:\")\n",
    "    print(df_final[~df_final[\"etiqueta\"].str[0].str.isupper()][\"etiqueta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vector_distance\"] = df[\"centroid\"].apply(lambda x: np.linalg.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los vectores\n",
    "df[\"centroid_normalized\"] = df[\"centroid\"].apply(lambda x: x / np.linalg.norm(x))\n",
    "\n",
    "# Calcular la nueva distancia del vector normalizado\n",
    "df[\"vector_distance_normalized\"] = df[\"centroid_normalized\"].apply(lambda x: np.linalg.norm(x))\n",
    "\n",
    "# Mostrar el DataFrame actualizado\n",
    "print(df[[\"cluster_label\", \"etiqueta\", \"centroid_normalized\", \"vector_distance_normalized\"]])\n",
    "\n",
    "# Verificar que todas las distancias normalizadas sean aproximadamente 1\n",
    "print(\"\\n¿Todas las distancias normalizadas son aproximadamente 1?\")\n",
    "print(np.allclose(df[\"vector_distance_normalized\"], 1.0, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "\n",
    "client = SearchClient(endpoint, index_name, AzureKeyCredential(key))\n",
    "results = list(client.search(search_text=\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open(\"documentos_pre.json\", \"w\") as f: \n",
    "#     json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coger docs de PRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "results = json.load(open(\"documentos_pre.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document_path', 'ts_file_update', 'category', 'description', 'original_canonic_path', 'content_vector', 'dt_end', 'document_id', 'file_size', 'calendar_name', 'location', 'site_name', 'sp', 'title', 'sheet_name', 'event_id', 'event_name', 'dt_start', 'content', 'etiqueta_cluster', 'event_url', 'ts_creation', 'original_path', 'content_md', 'document_name', 'chunk_id', 'content_type', 'id', 'ts_processed', 'page_number', '@search.score', '@search.reranker_score', '@search.highlights', '@search.captions'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>content_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayudadch_copilot_manuales_de_formacion_superme...</td>\n",
       "      <td>La tabla presenta una lista de productos relac...</td>\n",
       "      <td>[-0.029008377, 0.019819578, -0.0022607665, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ayudadch_copilot_manuales_de_formacion_superme...</td>\n",
       "      <td>La tabla presenta un horario de actividades o ...</td>\n",
       "      <td>[-0.023789804, 0.015766677, 0.0006295868, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>supermercadodch_departamento_formacion_gestion...</td>\n",
       "      <td>No hay casos abiertos por el mismo motivo Inic...</td>\n",
       "      <td>[-0.009540582, 0.03877473, -0.001934909, 0.023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>supermercadodch_departamento_formacion_gestion...</td>\n",
       "      <td>La tabla presenta un inventario de productos c...</td>\n",
       "      <td>[-0.016271949, 0.03131432, 0.00031013912, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ayudadch_copilot_manuales_de_formacion_superme...</td>\n",
       "      <td>Captura pedidos GPER DVD División de venta a d...</td>\n",
       "      <td>[-0.002767793, 0.031104341, -0.0032851375, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  ayudadch_copilot_manuales_de_formacion_superme...   \n",
       "1  ayudadch_copilot_manuales_de_formacion_superme...   \n",
       "2  supermercadodch_departamento_formacion_gestion...   \n",
       "3  supermercadodch_departamento_formacion_gestion...   \n",
       "4  ayudadch_copilot_manuales_de_formacion_superme...   \n",
       "\n",
       "                                             content  \\\n",
       "0  La tabla presenta una lista de productos relac...   \n",
       "1  La tabla presenta un horario de actividades o ...   \n",
       "2  No hay casos abiertos por el mismo motivo Inic...   \n",
       "3  La tabla presenta un inventario de productos c...   \n",
       "4  Captura pedidos GPER DVD División de venta a d...   \n",
       "\n",
       "                                      content_vector  \n",
       "0  [-0.029008377, 0.019819578, -0.0022607665, 0.0...  \n",
       "1  [-0.023789804, 0.015766677, 0.0006295868, 0.02...  \n",
       "2  [-0.009540582, 0.03877473, -0.001934909, 0.023...  \n",
       "3  [-0.016271949, 0.03131432, 0.00031013912, 0.03...  \n",
       "4  [-0.002767793, 0.031104341, -0.0032851375, 0.0...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)[[\"id\", \"content\", \"content_vector\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        # scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        # scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        # scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        # scores[\"Adjusted Rand-Index\"].append(\n",
    "        #     metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        # )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)\n",
    "\n",
    "    return evaluations, evaluations_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectores normalizados con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Normalizar los vectores en content_vector\n",
    "\n",
    "# Función para normalizar un vector\n",
    "def normalizar_vector(vector):\n",
    "    norma = np.linalg.norm(vector)\n",
    "    return vector / norma if norma != 0 else vector\n",
    "\n",
    "# Aplicar la normalización a cada vector en la columna content_vector\n",
    "df['content_vector_norm'] = df['content_vector'].apply(normalizar_vector)\n",
    "\n",
    "print(\"Vectores normalizados con éxito.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def cluster_kmeans(embeddings, k, n_init=5, max_iter=500):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=n_init, max_iter=max_iter)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    inertia = kmeans.inertia_  # Inercia\n",
    "    silhouette_avg = silhouette_score(embeddings, cluster_labels)  # Silhouette Score\n",
    "    return cluster_labels, inertia, silhouette_avg, kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INIT = 25\n",
    "MAX_N_CLUSTERS = 100\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "silhouettes = list()\n",
    "inertias = list()\n",
    "n_clusters = range(2, MAX_N_CLUSTERS)\n",
    "\n",
    "for n in tqdm(n_clusters):\n",
    "    final_cluster_labels, inertia, silhouette_avg, cluster_centers = cluster_kmeans(df[\"content_vector_norm\"].tolist(), n, n_init=N_INIT)\n",
    "    silhouettes.append(silhouette_avg)\n",
    "    inertias.append(inertia)\n",
    "\n",
    "# Plot the silhouettes\n",
    "plt.plot(n_clusters, silhouettes, marker=\"o\")\n",
    "plt.xlabel(\"Número de clusters\")\n",
    "plt.ylabel(\"Coeficiente de silueta\")\n",
    "plt.title(\"Coeficiente de silueta para diferentes números de clusters\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the inertias\n",
    "plt.plot(n_clusters, inertias, marker=\"o\")\n",
    "plt.xlabel(\"Número de clusters\")\n",
    "plt.ylabel(\"Inercia\")\n",
    "plt.title(\"Inercia para diferentes números de clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the silhouettes\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "ax.plot(n_clusters, silhouettes, marker=\"o\")\n",
    "ax.set_xlabel(\"Número de clusters\")\n",
    "plt.ylabel(\"Coeficiente de silueta\")\n",
    "plt.title(\"Coeficiente de silueta para diferentes números de clusters\")\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la inercia\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "ax.plot(n_clusters, inertias, marker=\"o\")\n",
    "ax.set_xlabel(\"Número de clusters\")\n",
    "ax.set_ylabel(\"Inercia\")\n",
    "ax.set_title(\"Inercia para diferentes números de clusters\")\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 37\n",
    "\n",
    "final_cluster_labels, inertia, silhouette_avg, cluster_centers = cluster_kmeans(df[\"content_vector_norm\"].tolist(), best_k, n_init=50, max_iter=500)\n",
    "df[\"cluster_label\"] = final_cluster_labels\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Score: \", silhouette_avg, \"Inertia: \", inertia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un gráfico de barras para visualizar el conteo de etiquetas de cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.cluster_label.value_counts().plot(kind='bar')\n",
    "plt.title('Distribución de Etiquetas de Cluster')\n",
    "plt.xlabel('Etiqueta de Cluster')\n",
    "plt.ylabel('Conteo')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de clusters usando PCA de dos dimensiones\n",
    "\n",
    "# Importar las bibliotecas necesarias\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Crear un objeto PCA con 2 componentes\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "pca_result = pca.fit_transform(df[\"content_vector_norm\"].tolist())\n",
    "\n",
    "# Crear un DataFrame con los resultados del PCA\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['cluster'] = df['cluster_label']\n",
    "\n",
    "# Crear una paleta de colores altamente diferenciables\n",
    "colores = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.CSS4_COLORS.values())\n",
    "n_clusters = len(pca_df['cluster'].unique())\n",
    "paleta_colores = colores[:n_clusters]\n",
    "\n",
    "# Crear un gráfico de dispersión\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['cluster'], cmap=mcolors.ListedColormap(paleta_colores), alpha=0.7)\n",
    "\n",
    "plt.title('Visualización de Clusters usando PCA')\n",
    "plt.xlabel('Primera Componente Principal')\n",
    "plt.ylabel('Segunda Componente Principal')\n",
    "\n",
    "# Añadir una barra de color\n",
    "plt.colorbar(scatter, label='Etiqueta de Cluster', ticks=range(n_clusters))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir la varianza explicada por cada componente\n",
    "print(f\"Varianza explicada por la PC1: {pca.explained_variance_ratio_[0]:.2f}\")\n",
    "print(f\"Varianza explicada por la PC2: {pca.explained_variance_ratio_[1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización interactiva de clusters usando PCA de tres dimensiones con Plotly\n",
    "\n",
    "# Importar las bibliotecas necesarias\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Crear un objeto PCA con 3 componentes\n",
    "pca_3d = PCA(n_components=3)\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "pca_result_3d = pca_3d.fit_transform(df[\"content_vector\"].tolist())\n",
    "\n",
    "# Crear un DataFrame con los resultados del PCA\n",
    "pca_df_3d = pd.DataFrame(data=pca_result_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_df_3d['cluster'] = df['cluster_label']\n",
    "\n",
    "# Definir la misma paleta de colores que en el gráfico anterior\n",
    "colores = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.CSS4_COLORS.values())\n",
    "n_clusters = len(pca_df_3d['cluster'].unique())\n",
    "paleta_colores = colores[:n_clusters]\n",
    "\n",
    "# Crear un diccionario de mapeo de cluster a color\n",
    "color_map = {str(cluster): color for cluster, color in zip(pca_df_3d['cluster'].unique(), paleta_colores)}\n",
    "\n",
    "# Crear un gráfico de dispersión 3D interactivo con Plotly\n",
    "fig = px.scatter_3d(pca_df_3d, x='PC1', y='PC2', z='PC3', \n",
    "                    color='cluster', \n",
    "                    color_discrete_map=color_map,\n",
    "                    title='Visualización Interactiva de Clusters usando PCA en 3D',\n",
    "                    labels={'PC1': 'Primera Componente Principal',\n",
    "                            'PC2': 'Segunda Componente Principal',\n",
    "                            'PC3': 'Tercera Componente Principal'},\n",
    "                    hover_data=['cluster'])\n",
    "\n",
    "# Ajustar el diseño del gráfico y reducir el tamaño de los puntos\n",
    "fig.update_layout(scene = dict(\n",
    "                    xaxis_title='Primera Componente Principal',\n",
    "                    yaxis_title='Segunda Componente Principal',\n",
    "                    zaxis_title='Tercera Componente Principal'),\n",
    "                  width=900, height=700,\n",
    "                  margin=dict(r=20, b=10, l=10, t=40))\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))  # Reducir el tamaño de los puntos\n",
    "\n",
    "# Mostrar el gráfico interactivo\n",
    "fig.show()\n",
    "\n",
    "# Imprimir la varianza explicada por cada componente\n",
    "print(f\"Varianza explicada por la PC1: {pca_3d.explained_variance_ratio_[0]:.2f}\")\n",
    "print(f\"Varianza explicada por la PC2: {pca_3d.explained_variance_ratio_[1]:.2f}\")\n",
    "print(f\"Varianza explicada por la PC3: {pca_3d.explained_variance_ratio_[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución distancias al centroide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histograms_and_identify_outliers(cluster_data, cluster_label):\n",
    "    # Calculate the centroid\n",
    "    centroid = np.mean(cluster_data, axis=0)\n",
    "    distances = cosine_distances(cluster_data, [centroid]).flatten()\n",
    "    \n",
    "    # Calculate the threshold for identifying outliers\n",
    "    threshold = np.percentile(distances, 95)  # Change the percentile if necessary\n",
    "    outliers = distances[distances > threshold]\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(distances, bins=30, alpha=0.7, color='blue')\n",
    "    plt.title(f'Histograma de distancias coseno al centroide - Cluster {cluster_label}')\n",
    "    plt.xlabel('Distancia Coseno')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid()\n",
    "    plt.axvline(x=np.mean(distances), color='red', linestyle='dashed', linewidth=1, label='Media')\n",
    "    plt.axvline(x=threshold, color='green', linestyle='dashed', linewidth=1, label='Umbral de Outliers')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the number of outliers and total embeddings\n",
    "    total_embeddings = len(cluster_data)\n",
    "    per_outliers= (len(outliers))*100/total_embeddings\n",
    "    per_outliers = round(per_outliers, 3)\n",
    "    print(f'Cluster {cluster_label}: {len(outliers)} outliers identificados de {total_embeddings} embeddings totales.')\n",
    "    print(f'Percentage of total outliers is {per_outliers}%.')\n",
    "\n",
    "# Generate histograms and count outliers for each cluster\n",
    "for label in df['cluster_label'].unique():\n",
    "    cluster_data = np.array(df[df['cluster_label'] == label]['content_vector_norm'].tolist())\n",
    "    plot_histograms_and_identify_outliers(cluster_data, label)\n",
    "\n",
    "print(\"Histogramas de distancias coseno al centroide generados y outliers identificados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Crear un objeto UMAP con 2 componentes\n",
    "umap_model = umap.UMAP(n_components=2)\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "umap_result = umap_model.fit_transform(np.array(df[\"content_vector_norm\"].tolist()))\n",
    "\n",
    "# Crear un DataFrame con los resultados de UMAP\n",
    "umap_df = pd.DataFrame(data=umap_result, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['cluster'] = df['cluster_label']\n",
    "\n",
    "# Crear una paleta de colores altamente diferenciables\n",
    "colores = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.CSS4_COLORS.values())\n",
    "n_clusters = len(umap_df['cluster'].unique())\n",
    "paleta_colores = colores[:n_clusters]\n",
    "\n",
    "# Crear un gráfico de dispersión\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(umap_df['UMAP1'], umap_df['UMAP2'], c=umap_df['cluster'], cmap=mcolors.ListedColormap(paleta_colores), alpha=0.7)\n",
    "\n",
    "plt.title('Visualización de Clusters usando UMAP')\n",
    "plt.xlabel('Primera Componente UMAP')\n",
    "plt.ylabel('Segunda Componente UMAP')\n",
    "\n",
    "# Añadir una barra de color\n",
    "plt.colorbar(scatter, label='Etiqueta de Cluster', ticks=range(n_clusters))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Crear un objeto UMAP con 3 componentes\n",
    "umap_model = umap.UMAP(n_components=3)\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "umap_result = umap_model.fit_transform(np.array(df[\"content_vector_norm\"].tolist()))\n",
    "\n",
    "# Crear un DataFrame con los resultados de UMAP\n",
    "umap_df = pd.DataFrame(data=umap_result, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
    "umap_df['cluster'] = df['cluster_label'].astype(str)  # Convertir a string para la visualización\n",
    "umap_df['id'] = df['id']\n",
    "\n",
    "# Crear una visualización 3D interactiva con Plotly\n",
    "fig = px.scatter_3d(umap_df, x='UMAP1', y='UMAP2', z='UMAP3', color='cluster',\n",
    "                    title='Visualización Interactiva de Clusters usando UMAP en 3D',\n",
    "                    labels={'UMAP1': 'Primera Componente UMAP',\n",
    "                            'UMAP2': 'Segunda Componente UMAP',\n",
    "                            'UMAP3': 'Tercera Componente UMAP'},\n",
    "                             text='id',  # Usar el id como etiqueta\n",
    "                    color_continuous_scale=px.colors.qualitative.Set1,\n",
    "                    width=1000,  # Ancho de la figura\n",
    "                    height=800)  # Alto de la figura\n",
    "\n",
    "# Mostrar la figura\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_tokens(texto, modelo=\"gpt-3.5-turbo\"):\n",
    "    # Cargar el codificador específico para el modelo\n",
    "    codificador = tiktoken.encoding_for_model(modelo)\n",
    "    # Convertir el texto en tokens\n",
    "    tokens = codificador.encode(texto)\n",
    "    # Contar la cantidad de tokens\n",
    "    cantidad_tokens = len(tokens)\n",
    "    return cantidad_tokens\n",
    "\n",
    "# Obtener todos los documentos de la base de datos\n",
    "def get_all_docs(search_client):\n",
    "    results = search_client.search(search_text=\"*\", top=None, include_total_count=True)\n",
    "    documents = [result for result in results]\n",
    "    return documents\n",
    "\n",
    "# Obtener todos los embeddings sin agrupar por document_id\n",
    "def get_all_embeddings(search_client):\n",
    "    results = get_all_docs(search_client)\n",
    "    print(f\"Embeddings tratados: {len(results)}\")\n",
    "    embeddings = []\n",
    "    document_ids = []  # Para mantener una referencia a los IDs de los documentos\n",
    "    document_paths = []  # Para mantener los document_path\n",
    "    contents = []  # Para mantener el contenido de cada documento\n",
    "    total_documents = len(results)\n",
    "\n",
    "    for doc in results:\n",
    "        document_id = doc.get('id')  # ID del documento (nuevo campo)\n",
    "        document_path = doc.get('document_path')  # Path del documento\n",
    "        embedding = doc.get('content_vector')  # Vector de embedding\n",
    "        content = doc.get('content')  # Contenido del documento\n",
    "\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)  # Añadir el embedding directamente a la lista\n",
    "            document_ids.append(document_id)  # Mantener un registro del document_id\n",
    "            document_paths.append(document_path)  # Mantener un registro del document_path\n",
    "            contents.append(content)  # Almacenar el contenido también\n",
    "\n",
    "    print(f'Número total de documentos en el estudio de cluster: {total_documents} documentos')\n",
    "    print(f'Número total de embeddings procesados: {len(embeddings)}')\n",
    "\n",
    "    return np.array(embeddings), document_ids, document_paths, contents  # Devolver también el contenido\n",
    "\n",
    "\n",
    "# Función para generar etiquetas llamando al modelo de GPT\n",
    "def generate_cluster_label(contents):\n",
    "    prompt = (\"Analiza el texto que te voy a proporcionar a continuación y genera dos o varias etiquetas que represente el contenido con un máximo de 5.\"\n",
    "        \"Quiero que la etiqueta sea con una dos o tres palabras para dar detalle, y que clasifique el contenido por temas. \"\n",
    "        \"No incluyas la palabra 'Etiqueta' en la respuesta y asegúrate de que no esté en formato de lista. \"\n",
    "        \" Etiqueta esta en lenguaje español.\\n\\n\"\n",
    "    )\n",
    "    combined_contents = \"\\n\".join(contents)  # Añadir los contenidos\n",
    "    total_tokens = contar_tokens(prompt) + contar_tokens(combined_contents)\n",
    "\n",
    "    if total_tokens > 125000:\n",
    "        # Si excede el límite, truncar o dividir los contenidos\n",
    "        combined_contents = \" \".join(contents)[:125000 - contar_tokens(prompt)]  # Truncar para ajustarse\n",
    "        contents = [combined_contents]  # Usar el contenido truncado\n",
    "\n",
    "    prompt += combined_contents  # Añadir el contenido truncado\n",
    "\n",
    "    # Llamar a la función call_chatgpt_model para obtener la etiqueta\n",
    "    etiqueta = call_chatgpt_model(prompt, model, assistant)\n",
    "    return etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Procesar los embeddings\n",
    "embeddings, document_ids, document_paths, contents = get_all_embeddings(search_client)  # Obtener también contenidos\n",
    "\n",
    "# Realizar el clustering final usando el mejor k\n",
    "final_cluster_labels, _, _, cluster_centers = cluster_kmeans(embeddings, 37)\n",
    "\n",
    "# Crear un diccionario para contar el número de embeddings en cada clúster\n",
    "clusters_count = {}\n",
    "clusters_dict = {}\n",
    "\n",
    "for label in final_cluster_labels:\n",
    "    if label not in clusters_count:\n",
    "        clusters_count[label] = 0\n",
    "        clusters_dict[label] = []  # Inicializa la lista para este cluster\n",
    "    clusters_count[label] += 1\n",
    "\n",
    "# Diccionario para almacenar las etiquetas de los clústeres\n",
    "cluster_labels = {}\n",
    "document_labels = []  # Lista para almacenar la info del document_id, document_path, content_vector y etiqueta\n",
    "\n",
    "for label, count in clusters_count.items():\n",
    "    print(f'Clúster {label}: {count} embeddings')\n",
    "\n",
    "    # Obtener los embeddings y el contenido en el clúster actual\n",
    "    cluster_embeddings = embeddings[final_cluster_labels == label]\n",
    "    cluster_ids = np.array(document_ids)[final_cluster_labels == label]  # Obtener document_id\n",
    "    cluster_paths = np.array(document_paths)[final_cluster_labels == label]  # Obtener document_path\n",
    "    cluster_texts = np.array(contents)[final_cluster_labels == label]  # Obtener contenidos\n",
    "\n",
    "    # Calcular la distancia de cada embedding al centroide del clúster\n",
    "    distances = cdist([cluster_centers[label]], cluster_embeddings, metric='euclidean')[0]\n",
    "\n",
    "    # Crear un diccionario para almacenar los documentos por clúster\n",
    "    clusters_dict[label] = []\n",
    "    for doc_id, embedding, labels, path in zip(document_ids, embeddings, final_cluster_labels, document_paths):\n",
    "        if labels not in clusters_dict:\n",
    "            clusters_dict[labels] = []\n",
    "        clusters_dict[labels].append((doc_id, embedding, path))\n",
    "    \n",
    "        # Generate a label for the cluster using GPT\n",
    "    etiqueta = generate_cluster_label(cluster_texts)\n",
    "\n",
    "    print(f'Etiqueta generada para el clúster {label}: {etiqueta} ')\n",
    "        \n",
    "        # Almacenar la etiqueta en el diccionario de clústeres\n",
    "    cluster_labels[label] = etiqueta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
